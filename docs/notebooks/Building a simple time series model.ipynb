{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the international airline passenger data available from [here](https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&display=line). This particular dataset is included with `creme` in the `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import datasets\n",
    "\n",
    "for x, y in datasets.load_airline():\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is as simple as can be: it consists of a sequence of months and values representing the total number of international airline passengers per month. Our goal is going to be to predict the number of passengers for the next month at each step. Notice that because the dataset is small  -- which is usually the case for time series -- we could just fit a model from scratch each month. However for the sake of example we're going to train a single model online. Although the overall performance might be potentially weaker, training a time series model has the benefit of being scalable if, say, you have have [thousands of time series to manage](http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html).\n",
    "\n",
    "We'll start with a very simple model where the only feature will be the [ordinal date](https://www.wikiwand.com/en/Ordinal_date) of each month. This should be able to capture some of the underlying trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import compose\n",
    "from creme import linear_model\n",
    "from creme import preprocessing\n",
    "\n",
    "\n",
    "def get_ordinal_date(x):\n",
    "    return {'ordinal_date': x['month'].toordinal()}\n",
    "\n",
    "\n",
    "model = compose.Pipeline([\n",
    "    ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write down a function to evaluate the model. This will go through each observation in the dataset and update the model as it goes on. The prior predictions will be stored along with the true values and will be plotted together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model(model): \n",
    "    \n",
    "    metric = metrics.MAE()\n",
    "    \n",
    "    dates = []\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "\n",
    "    for x, y in datasets.load_airline():\n",
    "        \n",
    "        # Obtain the prior prediction\n",
    "        y_pred = model.fit_predict_one(x, y)\n",
    "        \n",
    "        # Update the error metric\n",
    "        metric.update(y, y_pred)\n",
    "        \n",
    "        # Store the true value and the prediction\n",
    "        dates.append(x['month'])\n",
    "        y_trues.append(y)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.plot(dates, y_trues, lw=3, color='#2ecc71', alpha=0.8, label='Truth')\n",
    "    ax.plot(dates, y_preds, lw=3, color='#e74c3c', alpha=0.8, label='Prediction')\n",
    "    ax.legend()\n",
    "    ax.set_title(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has capture a trend but not the right one. Indeed it thinks the trend is linear whereas we can visually see that the growth of the data increases with time. In other words second derivative of the data is positive. This is a well know problem in time series forecasting and there are thus many ways to handle it; for example by using a [Box-Cox transform](https://www.wikiwand.com/en/Power_transform). However we are going to do something a bit unconventional and admittedly way cooler: we're going to tell our linear regression to calculate the intercept using a rolling mean. The thing is that `creme`'s linear regression implementation works differently than most implementations in that it doesn't use gradient descent to optimize the intercept. Instead the `LinearRegression` classes uses a running statistic which is by default the mean of the data. The trick is that we can plug in any other running statistic. In our case we'll use a `RollingMean(12)` so that the intercept will be equal to the last 12 values of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import stats\n",
    "\n",
    "\n",
    "model = compose.Pipeline([\n",
    "    ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(intercept=stats.RollingMean(12)))\n",
    "])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and capture the monthly trend by one-hot encoding the month name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "\n",
    "def get_month(x):\n",
    "    return {\n",
    "        calendar.month_name[month]: month == x['month'].month\n",
    "        for month in range(1, 13)\n",
    "    }\n",
    "    \n",
    "\n",
    "model = compose.Pipeline([\n",
    "    ('features', compose.TransformerUnion([\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month', compose.FuncTransformer(get_month)),\n",
    "    ])),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(intercept=stats.RollingMean(12)))\n",
    "])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty decent. We can take a look at the weights of the linear regression to get an idea of the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps[-1][1].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be expected the months of July and August have the highest weights because these are the months where people typically go on holiday abroad. The month of December has a low weight because this is a month of festivities in most of the Western world where people usually stay at home.\n",
    "\n",
    "Our model seems to understand which months are important, but it fails to see that the importance of each month grows multiplicatively as the years go on. In other words our model is too shy. We can fix this by increasing the learning rate of the `LinearRegression`'s optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import optim\n",
    "\n",
    "model = compose.Pipeline([\n",
    "    ('features', compose.TransformerUnion([\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month', compose.FuncTransformer(get_month)),\n",
    "    ])),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(\n",
    "        intercept=stats.RollingMean(12),\n",
    "        optimizer=optim.VanillaSGD(0.05)\n",
    "    ))\n",
    "])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look good! Naturally in production we would tune the learning rate, ideally in real-time.\n",
    "\n",
    "One thing that should bug you is that the first prediction is 0 whereas the true value is around 110. This is simply due to the fact that online models know virtually nothing when they begin, and so the first prediction of the `LinearRegression` is always 0. We can however fix this by providing an initial update to the running statistic that is used as an intercept. We will thus call the `update` method of the `RollingMean` so that the intercept starts at 110 instead of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compose.Pipeline([\n",
    "    ('features', compose.TransformerUnion([\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month', compose.FuncTransformer(get_month)),\n",
    "    ])),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(\n",
    "        intercept=stats.RollingMean(12).update(110),\n",
    "        optimizer=optim.VanillaSGD(0.05)\n",
    "    ))\n",
    "])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although setting the intercept this way works, it does feel a bit wonky. A more sophisticated approach might be to use a Bayesian model with some informative prior, but we won't delve into that right now.\n",
    "\n",
    "Before finishing, we're going to introduce a cool feature extraction trick based on [radial basis function kernel](https://www.wikiwand.com/en/Radial_basis_function_kernel). The one-hot encoding we did on the month is a good idea but if you think about it is a bit rigid. Indeed the value of each feature is going to be 0 or 1, depending on the month of each observation. We're basically saying that the month of September is as distant to the month of August as it is to the month of March. Of course this isn't true, and it would be nice if our features would reflect this. To do so we can simply calculate the distance between the month of each observation and all the months in the calendar. Instead of simply computing the distance linearly, we're going to use a so-called *Gaussian radial basic function kernel*. This is a bit of a mouthful but for us it boils down to a simple formula, which is:\n",
    "\n",
    "$$d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2})$$\n",
    "\n",
    "Intuitively this computes a similarity between two months -- denoted by $i$ and $j$ -- which decreases the further apart they are from each other. The $sigma$ parameter can be seen as a hyperparameter than can be tuned -- in the following snippet we'll simply ignore it. The thing to take away is that this results in smoother predictions than when using a one-hot encoding scheme, which is often a desirable property. You can also see trick in action [in this nice presentation](http://www.youtube.com/watch?v=68ABAU_V8qI&t=4m45s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_month_distances(x):\n",
    "    return {\n",
    "        calendar.month_name[month]: math.exp(-(x['month'].month - month) ** 2)\n",
    "        for month in range(1, 13)\n",
    "    }\n",
    "    \n",
    "\n",
    "model = compose.Pipeline([\n",
    "    ('features', compose.TransformerUnion([\n",
    "        ('ordinal_date', compose.FuncTransformer(get_ordinal_date)),\n",
    "        ('month_distances', compose.FuncTransformer(get_month_distances)),\n",
    "    ])),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression(\n",
    "        intercept=stats.RollingMean(12).update(110),\n",
    "        optimizer=optim.VanillaSGD(0.05)\n",
    "    ))\n",
    "])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've managed to get a good looking prediction curve with a reasonably simple model. What's more our model has the advantage of being interpretable and easy to debug. There surely are more rocks to squeeze (e.g. tune the hyperparameters, use an ensemble model, etc.) but we'll leave that as an exercice to the reader.\n",
    "\n",
    "As a finishing touch we'll rewrite the final pipeline with a less verbose style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compose.TransformerUnion([\n",
    "            get_ordinal_date,\n",
    "            get_month_distances,\n",
    "        ]) + \\\n",
    "        preprocessing.StandardScaler() + \\\n",
    "        linear_model.LinearRegression(\n",
    "            intercept=stats.RollingMean(12).update(110),\n",
    "            optimizer=optim.VanillaSGD(0.05)\n",
    "        )\n",
    "\n",
    "evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
